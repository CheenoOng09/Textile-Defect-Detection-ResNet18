# Textile Defect Detection (Smart Manufacturing Project)

This repository contains my **Smart Manufacturing** class project: detecting textile defects using a **ResNet** classifier trained on grayscale patch datasets (32Ã—32 and 64Ã—64). Code supports building datasets from CSV+HDF5 pairs, training/evaluation with PyTorch, and running inference on new images.

- Slides: `docs/Smart Manufacturing Presentation.pdf` (overview, baselines, metrics) îˆ€fileciteîˆ‚turn1file0îˆ
- Report: `docs/Smart Manufacturing Project Report.pdf` (training results, overfitting discussion, datasets used) îˆ€fileciteîˆ‚turn1file1îˆ

## ğŸ§  Approach (summary)
- **Model:** ResNet18 adapted for 1â€‘channel input; tested crossâ€‘entropy vs focal loss; Adam and SGD optimizers; LR scheduler (ReduceLROnPlateau) in the optimized loop. îˆ€fileciteîˆ‚turn1file2îˆ îˆ€fileciteîˆ‚turn1file3îˆ
- **Data:** CSV files provide `index`, `indication_value` (0=good, 1=defect after binarization), and `angle` metadata; HDF5 files store image tensors. The loader builds `x_train/x_val/x_test` by mapping CSV indices to HDF5 datasets. îˆ€fileciteîˆ‚turn1file2îˆ îˆ€fileciteîˆ‚turn1file3îˆ
- **Results:** Validation accuracy ~**87%** with Adam @ 100 epochs on 32Ã—32 patches; signs of **overfitting** on outâ€‘ofâ€‘distribution images; suggestions include stronger regularization/augmentation and checking preprocessing/labels.

## ğŸ“‚ Repo structure
```
.
â”œâ”€ src/
â”‚  â”œâ”€ training_script.py          # Optimized ResNet training loop (PyTorch) îˆ€fileciteîˆ‚turn1file3îˆ
â”‚  â””â”€ smartman_1.py               # Earlier endâ€‘toâ€‘end notebook export (CSV+H5 to train/test) îˆ€fileciteîˆ‚turn1file2îˆ
â”œâ”€ docs/
â”‚  â”œâ”€ Smart Manufacturing Presentation.pdf
â”‚  â””â”€ Smart Manufacturing Project Report.pdf
â”œâ”€ data/                          # (gitignored) place CSV/HDF5 here if you keep them locally
â”‚  â”œâ”€ train32.csv / test32.csv
â”‚  â”œâ”€ train32.h5 / test32.h5
â”‚  â”œâ”€ train64.csv / test64.csv
â”‚  â””â”€ train64.h5 / test64.h5
â”œâ”€ models/                        # (LFS) trained weights & metrics
â”‚  â”œâ”€ best_model.pth
â”‚  â””â”€ metrics.pth
â””â”€ README.md
```

> **Large files:** The raw datasets and weights are big. This repo uses **Git LFS** for `*.pth` and ignores `data/` by default. If you need to share datasets, link to their source (e.g., Kaggle/ZJU Leaper) rather than committing binaries. 

## âš™ï¸ Environment (Windows + Anaconda)
From the project report/setup notes: create an environment, install PyTorch (with CUDA if available), and common libs. îˆ€fileciteîˆ‚turn1file4îˆ

```bash
# Create env
conda create -n sm-textiles python=3.9 -y
conda activate sm-textiles

# Install PyTorch (CUDA 11.8 example; adjust to your GPU/CPU)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Others
conda install pandas h5py matplotlib scikit-learn pillow -y
pip install keras  # needed for one-hot utilities used in scripts
```

## â–¶ï¸ Training / Evaluation
Update the paths in `src/training_script.py` to point to your CSV/H5 files (see comments in the file). It expects CSVs with columns including `index`, `indication_value`, and `angle`, and HDF5 with an `'images'` dataset (verify your key per notes). 

```bash
# Train (saves best_model.pth and metrics.pth to models/)
python src/training_script.py
```

To run the earlier endâ€‘toâ€‘end pipeline with focal loss/Adam and quick inference examples, see `src/smartman_1.py`. îˆ€fileciteîˆ‚turn1file2îˆ

## ğŸ—ƒï¸ About the large files (what they are & how they affect the project)
- **`matchingtDATASET_train_32.h5`, `matchingtDATASET_test_32.h5`, `..._64.h5`**  
  HDF5 containers holding grayscale image patches (32Ã—32 or 64Ã—64). The scripts load arrays from these files using a dataset key (often `'images'`) and pair them to rows in the CSV via the `index` column. They are required to actually build the `x_train/x_val/x_test` tensors. 
- **`train32.csv`, `test32.csv`, `train64.csv`, `test64.csv`**  
  Metadata tables used to **select rows** from HDF5 and provide labels: `indication_value` is binarized (0â†’good, â‰ 0â†’defect), and `angle` is used to filter/concatenate subsets (e.g., only 20Â°/120Â° angles in some experiments). Uploading CSVs alone is not enough to run training, but they are small and **useful to include** for transparency and to document the schema. îˆ€fileciteîˆ‚turn1file2îˆ îˆ€fileciteîˆ‚turn1file3îˆ
- **`best_model.pth`**  
  PyTorch model weights saved when validation accuracy improves. Needed for inference without retraining; store via **Git LFS** (binary). îˆ€fileciteîˆ‚turn1file3îˆ
- **`metrics.pth`**  
  A small PyTorch checkpoint containing lists of `train_losses`, `val_losses`, `train_accuracies`, `val_accuracies` so you can reâ€‘plot curves without reâ€‘training. Optional but nice to keep under `models/` (LFS is OK but not strictly required if itâ€™s small). 

### Should you upload the CSVs?
**Yes**, include the CSVs (theyâ€™re small) so others can inspect the splits/labels and replicate indexing. Keep all the `*.h5` datasets **out** of the repo; link to their sources or a cloud download instead. Your code will still run if the user downloads the H5 files locally and sets `file_path` accordingly. îˆ€fileciteîˆ‚turn1file1îˆ

## â¬†ï¸ Git LFS & .gitignore
This repo is configured to:
- track large binaries (`*.pth`) with **Git LFS**,
- ignore datasets (`data/**`) to keep the repo light.

### LFS (first time per machine)
```bash
git lfs install
git lfs track "*.pth"
git add .gitattributes
```

## ğŸ“Œ Repro tips
- Verify HDF5 keys before training (print `list(f.keys())` and use that key in the loader). îˆ€fileciteîˆ‚turn1file4îˆ
- Ensure GPU is detected (`torch.cuda.is_available()`); training speed and batch size depend on it. îˆ€fileciteîˆ‚turn1file4îˆ
- If you see overfitting, try stronger augmentation, `weight_decay`, `dropout`, and review labeling/normalization. îˆ€fileciteîˆ‚turn1file1îˆ

## ğŸ“„ License
Educational use.
